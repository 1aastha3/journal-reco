,abstract,title,pub_name,url,score
37,"Physics-based models are widely used to study dynamical systems in a variety of scientific and engineering problems. However, these models are necessarily approximations of reality due to incomplete knowledge or excessive complexity in modeling underlying processes. As a result, they often produce biased simulations due to inaccurate parameterizations or approximations used to represent the true physics. In this paper, we aim to build a new physics-guided machine learning framework to monitor dynamical systems. The idea is to use advanced machine learning model to extract complex spatio-temporal data patterns while also incorporating general scientific knowledge embodied in simulated data generated by the physics-based model. To handle the bias in simulated data caused by imperfect parameterization, we propose to extract general physical relations jointly from multiple sets of simulations generated by a physics-based model under different physical parameters. In particular, we develop a spatio-temporal network architecture that uses its gating variables to capture the variation of physical parameters. We initialize this model using a pre-training strategy that helps discover common physical patterns shared by different sets of simulated data. Then, we fine-tune it combining limited observations and adequate simulations. By leveraging the complementary strength of machine learning and domain knowledge, our method has been shown to produce accurate predictions, use less training samples and generalize to out-of-sample scenarios. We further show that the method can provide insights about the variation of physical parameters over space and time in two domain applications: predicting temperature in streams and predicting temperature in lakes.",Physics-guided machine learning from simulated data with different physical parameters,Knowledge and Information Systems,http://dx.doi.org/10.1007/s10115-023-01864-z,2.145895337283146
0,"At first glance, psychology and physics seem to have nothing in common. However, in both disciplines, sections are dealing with the study of one of the most complex objects in the universe: the human brain. Within psychology, neuropsychologists and neuroscientists “watch the brain at work” by measuring electrical and magnetic signals while participants perform tasks. Physics comprises the section of biophysics and is involved in the interdisciplinary field of computational neuroscience , in which the complex processes in the brain are represented with mathematical models. Furthermore, both disciplines focus on quantitative measurements and presenting their results in form of numbers.",Psychology and Physics: A Non-invasive Approach to the Functioning of the Human Mirror Neuron System,Measurement and Understanding in Science and Humanities,http://dx.doi.org/10.1007/978-3-658-36974-3_12,1.8096613951470697
1,"United in their motivation to better understand and describe the world, scientists from a wide range of disciplines search for new insights every day. At first glance, the approaches chosen on the way to gaining scientific knowledge seem to be very different. However, terms such as “number”, “pattern” or “model” appear in the communication of results in almost all fields of science. This suggests that nowadays a quantifying approach that focuses on “numbers” and “measures” is of particular importance.",Conclusion: Measuring and Understanding the World Through Science,Measurement and Understanding in Science and Humanities,http://dx.doi.org/10.1007/978-3-658-36974-3_18,0.7288745053857196
6,"This quote by William Thomson, Baron Kelvin, from a speech in 1883, which is still widely used today, can be cited in the context of debates surrounding the relationship between qualitative and quantitative approaches in science and their intrinsic value, as an unequivocal plea for the primacy of the latter. Furthermore, if one follows Kelvin’s argumentation, quantifiability is to be equated with scientificity.","Science, Numbers and Power: Contemporary Politics Between the Imperative of Rationalization and the Dependence on Numbers",Measurement and Understanding in Science and Humanities,http://dx.doi.org/10.1007/978-3-658-36974-3_9,0.6431855484277406
3,"Science and humanities divide the world, but also unite it. This attitude is the only one that brings progress and has the goal of creating an understanding of the world. This is reflected in our collaboration for this volume. Each project brings its own particular view of the roles of the concept of “number,” “measurement,” and “pattern” in understanding the world in each discipline considered. As different as these views are, there are also many similarities. In particular, three basic sets of themes can be identified that connect many of the individual contributions: first, the juxtaposition, the fusion, and the cross-fertilization of qualitative and quantitative approaches; second, the application, the interpretation, and the critical classification of mathematical and computational methods; and third, the quantification of different areas of life and science as a subject of inquiry.",Brief Presentation of the Contributions,Measurement and Understanding in Science and Humanities,http://dx.doi.org/10.1007/978-3-658-36974-3_2,0.49408779638927425
7,"The preceding parts presented the path from the desire for knowledge via methods, fundamentals and the inherent use of numbers and patterns to gaining knowledge for various disciplines and projects. This chapter deals with the communication cultures in sciences and humanities. Here, the focus is not only on a scientific publication as the “Endprodukt der Erkenntnisgewinnung”. On the one hand, this wider focus is chosen because science communication consists of more aspects than a scientific publication in the narrow sense and, on the other hand, the publication can serve as a starting point for new value chains of knowledge. The first part of this chapter reflects goals and methods of science communication, while the second part deals with current trends in knowledge communication and their consequences, and discusses the necessity, potentials and limitations of interdisciplinary projects.",Communication Cultures,Measurement and Understanding in Science and Humanities,http://dx.doi.org/10.1007/978-3-658-36974-3_17,0.48648337379868334
4,"Numerical simulations are used for the approximate prediction of situations under strictly defined conditions. They are based on mathematical models and represent interdependencies in the form of algorithms and computer programs. In everyday life, they are now ubiquitous for everyone, for example in weather forecasts or economic growth forecasts. In politics, they serve as an important tool for decision-making. In the scientific context, however, simulations are much more than a prediction tool. Similar to experiments, they also serve to build models themselves and thus enable the elucidation of causal relationships. Due to increasingly powerful computers, the importance of numerical simulation in science has grown steadily and rapidly over the last 50 years. In the meantime, it is considered an indispensable tool for gaining knowledge in many scientific disciplines. It is to be expected that numerical simulation will continue to grow rapidly in importance in the future and produce further surprising findings and technolo gies.",Through Numerical Simulation to Scientific Knowledge,Measurement and Understanding in Science and Humanities,http://dx.doi.org/10.1007/978-3-658-36974-3_16,0.44181429124524385
5,"The name econometrics is derived from the ancient Greek word combination: oikonomia = economy and metron = measure, measurement. Econometrics can, therefore, be described as the measurement of economic activity. Formally, econometrics is a social science that uses economic theory and mathematical methods as well as statistical data to quantitatively analyze economic processes and phenomena and empirically test economic theories. In the competition among different economic theoretical hypotheses, econometrics confronts these hypotheses with the reality provided by the data, so that the usefulness of an economic theoretical statement becomes tangible only if it can describe more or less the reality. An accurate description of the reality by an econometric model requires an accurate measurement of the relevant variables and their relationships.",Measuring and Understanding Financial Risks: An Econometric Perspective,Measurement and Understanding in Science and Humanities,http://dx.doi.org/10.1007/978-3-658-36974-3_10,0.44089671298003663
2,"When scientists from different research disciplines work together, this is initially associated with increased effort for everyone involved—common goals and terminology have to be found, perhaps even prejudices have to be overcome. Is all this work worthwhile, given that there are enough unsolved problems in each discipline? Here, we aim to illustrate the opportunities and advantages that interdisciplinary research brings, the specific challenges that have to be overcome, and how, in the experience of the authors of this book, successful research between disciplines can succeed.",Introduction,Measurement and Understanding in Science and Humanities,http://dx.doi.org/10.1007/978-3-658-36974-3_1,0.39846762889277004
8,Thermal comfort and thermal pain are two independent phenomena studied by different disciplines. The research field of thermal comfort deals with the adaptation of perception of thermal conditions at the workplace. That of thermal pain with sensory and emotional perceptions of painful heat stimuli and its adaptation.,Reflections and Perspectives on the Research Fields of Thermal Comfort at Work and Pain,Measurement and Understanding in Science and Humanities,http://dx.doi.org/10.1007/978-3-658-36974-3_13,0.39846762889277004
17,"Dense fibrous extracellular constitution of solid tumors exerts high resistance to diffusive transport into it; additionally, the scarcity of blood and lymphatic flows hinders convection. The complexity of fluidic transport mechanisms in such tumor environments still presents open questions with translational end goals. For example, clinical diagnosis and targeted drug delivery platforms for such dense tumors can ideally benefit from a quantitative framework on plasma uptake into the tumor. In this study, we present a computational model for physical parameters that may influence blood percolation and penetration into simple biomimetic solid tumor geometry. The model implements three-phase viscous-laminar transient simulation to mimic the transport physics inside a tumor-adhering blood vessel and measures the constituent volume fractions of the three considered phases, viz. plasma, RBCs (red blood cells, also known as “erythrocytes”), and WBCs (white blood cells, also known as “leukocytes”) at three different flow times, while simultaneously recording the plasma pressure and velocity at the entry point to the tumor’s extracellular space. Subsequently, to quantify plasma perfusion within the tumor zone, we proposed a reduced-order two-dimensional transport model for the tumor entry zone and its extracellular space for three different fenestra diameters: 0.1, 0.3, and 0.5 µm; the simulations were two-phase viscous-laminar transient. The findings support the hypothesis that plasma percolation into the tumor is proportional to the leakiness modulated by the size of fenestra openings, and the rate of percolation decays with the diffusion distance.",Development of a multiphase perfusion model for biomimetic reduced-order dense tumors,Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0150-x,0.16566356069585034
9,"In the context of deformable bubbles, surface tension produces a dynamic exchange between kinetic and surface elastic energy. This exchange of energy is relevant to bubble dynamics, like bubble induced turbulence or drag reduction. Unfortunately, the underlying physical mechanism is not exactly explained by the state-of-the-art numerical methods. In particular, the numerical violation of energy conservation results in an uncontrolled evolution of the system and yields well-known numerical pathologies. To remedy these problems, we tackle two of the most problematic terms in the numerical formulation: convection and surface tension. We identify the key mathematical identities that imply both physical conservation and numerical stability, present a semi-discretization of the problem that is fully energy preserving, and assess their stability in terms of the discrete energy contributions. Numerical experiments showcase the stability of the method and its energy evolution for stagnant and oscillating inviscid bubbles. Results show robust as well as bounded dynamics of the system, representing the expected physical mechanism.",Conservation of energy in the direct numerical simulation of interface-resolved multiphase flows,Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0148-4,0.11061182466079111
18,"As an emerging mathematical tool, inverse variational problem approximation (IVPA) has some real applications. Recently, deep learning is used to detect physical phenomena and carry out various tasks, which has achieved promising results. Several works have shown that can learn inverse variational problems (IVPs). In this work, we proposed a new framework to solve mean-curvature-based regularization IVPs from noisy data with different levels based on physical constrained learning and automatic differentiation. Furthermore, traditional variational methods and neural networks-based approaches are integrated, to learn the non-convex and high nonlinear inverse variational problems. And several experiments show that the effectiveness and robustness of our algorithm.",Learning mean curvature-based regularization to solve the inverse variational problems from noisy data,"Signal, Image and Video Processing",http://dx.doi.org/10.1007/s11760-023-02544-9,0.10480847260598208
46,"Abstract Statistical data summarization can significantly reduce the data storage footprint for large-scale scientific simulations while maintaining data accuracy. However, the high-resolution reconstructed data causes a memory bottleneck in graphics processing unit (GPU)-based post-hoc visualization using limited graphics memory. In this paper, we propose a statistical summarization model-driven adaptive data reconstruction method for large-scale statistical visualization on GPUs. It uses the spatial Gaussian mixture model to iteratively compute the Shannon entropy on multi-level grids, driving an adaptive mesh refinement that retains complex physical features. A graphics shader-based data reconstruction algorithm is used to efficiently generate the scalar field on the adaptive grid while seamlessly integrating with GPU-accelerated rendering algorithms. The experimental tests used data generated by five real-world scientific simulations with a maximum grid resolution of 134 million. Qualitative and quantitative analysis results show that our method can achieve efficient and high-quality reconstruction of the statistical summary data on a GPU, and the maximum data compression ratio is close to two orders of magnitude. Graphical abstract ",GPU-based adaptive data reconstruction for large-scale statistical visualization,Journal of Visualization,http://dx.doi.org/10.1007/s12650-022-00892-1,0.03678959484292129
27,"It is well known that as the generalized case for Fourier transform (FT) and fractional Fourier transform (FrFT), the linear canonical transform (LCT) has attracted more and more attention in signal processing and optics and so on due to more freedoms. On the other hand, as the tool of turning the real signals into the complex ones, the Hilbert transform (HT) is of much signification to information science. Since the complex signals through HT have the characteristic of the absence in negative frequency, HT plays an important role in communication. In the present paper, first, the generalized HT (GHT) in terms of LCT is given. Then, five types of generalized Uncertainty relations with novel uncertainty bounds of the complex signals in terms of generalized HT (GHT) in terms of LCT are demonstrated. These newly refined uncertainty bounds proved to be different from and much lower or sharper in many cases than that of the traditional complex and real signals. Finally, the examples are given to show the efficiency of the proposed uncertainty relations in this paper.",Uncertainty relations of the complex signals from generalized Hilbert transform on LCT,"Signal, Image and Video Processing",http://dx.doi.org/10.1007/s11760-023-02486-2,0.033213657366228715
22,"Intrinsic image decomposition is an important and long-standing computer vision problem. Given an input image, recovering the physical scene properties is ill-posed. Several physically motivated priors have been used to restrict the solution space of the optimization problem for intrinsic image decomposition. This work takes advantage of deep learning, and shows that it can solve this challenging computer vision problem with high efficiency. The focus lies in the feature encoding phase to extract discriminative features for different intrinsic layers from an input image. To achieve this goal, we explore the distinctive characteristics of different intrinsic components in the high-dimensional feature embedding space. We define feature distribution divergence to efficiently separate the feature vectors of different intrinsic components. The feature distributions are also constrained to fit the real ones through a feature distribution consistency. In addition, a data refinement approach is provided to remove data inconsistency from the Sintel dataset, making it more suitable for intrinsic image decomposition. Our method is also extended to intrinsic video decomposition based on pixel-wise correspondences between adjacent frames. Experimental results indicate that our proposed network structure can outperform the existing state-of-the-art.",Discriminative feature encoding for intrinsic image decomposition,Computational Visual Media,http://dx.doi.org/10.1007/s41095-022-0294-4,0.032392634974203555
32,"Commuter buses have a high passenger density relative to the interior cabin volume, and it is difficult to maintain a physical/social distance in terms of airborne transmission control. Therefore, it is important to quantitatively investigate the impact of ventilation and air-conditioning in the cabin on the airborne transmission risk for passengers. In this study, comprehensive coupled numerical simulations using computational fluid and particle dynamics (CFPD) and computer-simulated persons (CSPs) were performed to investigate the heterogeneous spatial distribution of the airborne transmission risk in a commuter bus environment under two types of layouts of the ventilation system and two types of passenger densities. Through a series of particle transmission analysis and infection risk assessment in this study, it was revealed that the layout of the supply inlet/exhaust outlet openings of a heating, ventilation, and air-conditioning (HVAC) system has a significant impact on the particle dispersion characteristics inside the bus cabin, and higher infection risks were observed near the single exhaust outlet in the case of higher passenger density. The integrated analysis of CFPD and CSPs in a commuter bus cabin revealed that the airborne transmission risk formed significant heterogeneous spatial distributions, and the changes in air-conditioning conditions had a certain impact on the risk.",Spatial distributions of airborne transmission risk on commuter buses: Numerical case study using computational fluid and particle dynamics with computer-simulated persons,Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0146-6,0.02966134678166744
34,"Optimization problems are often subject to various kinds of inexactness or inaccuracy of input data. Here, we consider multiobjective linear programming problems, in which two kinds of input entries have the form of interval data. First, we suppose that the objectives entries are interval values, and, second, we suppose that we have an interval estimation of weights of the particular criteria. These two types of interval data naturally lead to various definitions of efficient solutions. We discuss six meaningful concepts of efficient solutions and compare them to each other. For each of them, we attempt to characterize the corresponding kind efficiency and investigate computational complexity of deciding whether a given solution is efficient.",Various approaches to multiobjective linear programming problems with interval costs and interval weights,Central European Journal of Operations Research,http://dx.doi.org/10.1007/s10100-022-00804-6,0.0
36,"Due to the growth of wireless sensors, data loggers, and signal processing technologies, there has been a growing interest in the detection of damage to civil engineering infrastructures during the previous two decades. However, several challenges are encountered when applying this method to real civil engineering structures in operating and environmental conditions. The literature reviewed in this comprehensive review of vibration-based damage detection (VBDD) methods focuses on the long-term health monitoring of highway bridges. After providing a succinct summary of structural damage, damage detection methods, and the VBDD method, this article presents a state-of-the-art review of the challenges in using vibration-based damage detection methods on civil engineering infrastructures. Excitation techniques, modal parameter identification, data collection time, data communication, model-based damage detection, data-based damage detection, problems with handling big data, temperature influence, and boundary condition effects are some of the difficulties that might be taken into account for future research needs and directions. The article concludes that mode shape-based damage identification requires several dozen measurement locations, which is too complicated for the experimental study procedure due to the complexity of deployed surface-attached sensor networks, and the reliability of the ambient excitation method for large civil engineering infrastructures for early-stage damage detection is still in question.",Challenges in applying vibration-based damage detection to highway bridge structures,Asian Journal of Civil Engineering,http://dx.doi.org/10.1007/s42107-023-00594-5,0.0
35,"The paper provides a short history of the operations research (OR) in Slovenia. Some major events and achievements are mentioned and briefly discussed. The period starts in the year 1964, with the first symposium on OR in Slovenia. In the following decades, there were several important milestones: (1) the start of master’s and Ph.D. studies in OR in 1974, (2) the establishment of SSI-SSOR in 1992 (SSI-SSOR stands for the Slovenian Section for Operational Research within the Slovenian Society of Informatics), and (3) the start of a series of symposia in operations research in 1993. All these activities resulted in an extensive list of publications, projects, and monographs and international connections, proving that OR is still a vibrant field, which facilitates knowledge transfer from pure research to business applications.",60 years of OR in Slovenia: development from a first conference to a vibrant community,Central European Journal of Operations Research,http://dx.doi.org/10.1007/s10100-023-00859-z,0.0
38,"The continuous development of metal additive manufacturing (AM) promises the flexible and customized production, spurring AM research towards end-use part fabrication rather than prototyping, but inability to well control process defects and variability has precluded the widespread applications of AM. To solve these issues, process monitoring and control is a powerful approach. Recently, a variety of monitoring methods have been proposed and integrated with metal AM machines, which enables a large volume of data to be collected during the process. However, the data analytics faces great challenges due to the complexity of the process, bringing difficulties on developing effective models for defects detection as well as feedback control to improve quality. To overcome these challenges, machine learning methods have been frequently employed in the model development. By using machine learning methods, the models can be built based on the collected dataset, while it is not necessary to fully understand the process. This paper reviews the applications of machine learning methods in metal powder-bed fusion process monitoring and control, illuminates the challenges to be solved, and outlooks possible solutions.",Applications of machine learning in metal powder-bed fusion in-process monitoring and control: status and challenges,Journal of Intelligent Manufacturing,http://dx.doi.org/10.1007/s10845-022-01972-7,0.0
43,"Let $$\Omega $$ Ω be a domain in $${\mathbb {R}}^n$$ R n , $$\Gamma $$ Γ be a hyperplane intersecting $$\Omega $$ Ω , $$\varepsilon >0$$ ε > 0 be a small parameter, and $$D_{k,\varepsilon }$$ D k , ε , $$k=1,2,3\dots $$ k = 1 , 2 , 3 ⋯ be a family of small “holes” in $$\Gamma \cap \Omega $$ Γ ∩ Ω ; when $$\varepsilon \rightarrow 0$$ ε → 0 , the number of holes tends to infinity, while their diameters tends to zero. Let $${\mathscr {A}}_\varepsilon $$ A ε be the Neumann Laplacian in the perforated domain $$\Omega _\varepsilon =\Omega \setminus \Gamma _\varepsilon $$ Ω ε = Ω \ Γ ε , where $$\Gamma _\varepsilon =\Gamma \setminus (\cup _k D_{k,\varepsilon })$$ Γ ε = Γ \ ( ∪ k D k , ε ) (“sieve”). It is well-known that if the sizes of holes are carefully chosen, $${\mathscr {A}}_\varepsilon $$ A ε converges in the strong resolvent sense to the Laplacian on $$\Omega \setminus \Gamma $$ Ω \ Γ subject to the so-called $$\delta '$$ δ ′ -conditions on $$\Gamma \cap \Omega $$ Γ ∩ Ω . In the current work we improve this result: under rather general assumptions on the shapes and locations of the holes we derive estimates on the rate of convergence in terms of $$L^2\rightarrow L^2$$ L 2 → L 2 and $$L^2\rightarrow H^1$$ L 2 → H 1 operator norms; in the latter case a special corrector is required.",Operator estimates for the Neumann sieve problem,Annali di Matematica Pura ed Applicata (1923 -),http://dx.doi.org/10.1007/s10231-023-01308-z,0.0
39,"Lubricants are often contaminated by water in different ways. Water-polluted lubricants extremely accelerate wear corrosion, leading to the deterioration of lubricity performance. Recently, multiphase media superwettability has been developed to endow one surface with compatible functions, such as on-demand separation of oily wastewater. However, realizing the robustness of the dual superlyophobic surface to solve water-caused lubricant deterioration and water contamination as needed remains challenges. Herein, a robust dual superlyophobic membrane is presented to realize on-demand separation for various lubricant—water emulsions. Compared to pure lubricants, the purified lubricants have equivalent tribology performance, which are much better than that of water-polluted lubricants. The as-prepared membrane maintains dual superlyophobicity, high-efficient for water or lubricant purification, and excellent tribology performance of the purified lubricant, even after immersion in hot liquids for 24 h, multicycle separation, and sandpaper abrasion for 50 cycles. Water-polluted lubricant extremely accelerates wear corrosion to promote catalytic dehydrogenation of lubricants, generating too much harmful carbon-based debris. This work shows great guiding significance for recovering the tribology performance of water-polluted lubricants and purifying water by the dual superlyophobic membrane.",A robust membrane with dual superlyophobicity for solving water-caused lubricant deterioration and water contamination,Friction,http://dx.doi.org/10.1007/s40544-022-0677-7,0.0
40,"We study the friction when a rectangular tire tread rubber block is sliding on an ice surface at different temperatures ranging from −38 to −2 °C, and sliding speeds ranging from 3 µm/s to 1 cm/s. At low temperatures and low sliding speeds we propose that an important contribution to the friction force is due to slip between the ice surface and ice fragments attached to the rubber surface. At temperatures above −10 °C or for high enough sliding speeds, a thin premelted water film occurs on the ice surface and the contribution to the friction from shearing the area of real contact is small. In this case the dominant contribution to the friction force comes from viscoelastic deformations of the rubber by the ice asperities. We comment on the role of waxing on the friction between skis and snow (ice particles).",Rubber-ice friction,Friction,http://dx.doi.org/10.1007/s40544-022-0715-5,0.0
41,"Additive manufacturing (AM) offers the advantage of producing complex parts more efficiently and in a lesser production cycle time as compared to conventional subtractive manufacturing processes. It also provides higher flexibility for diverse applications by facilitating the use of a variety of materials and different processing technologies. With the exceptional growth of computing capability, researchers are extensively using machine learning (ML) techniques to control the performance of every phase of AM processes, such as design, process parameters modeling, process monitoring and control, quality inspection, and validation. Also, ML methods have made it possible to develop cybermanufacturing for AM systems and thus revolutionized Industry 4.0. This paper presents the state-of-the-art applications of ML in solving numerous problems related to AM processes. We give an overview of the research trends in this domain through a systematic literature review of relevant journal articles and conference papers. We summarize recent development and existing challenges to point out the direction of future research scope. This paper can provide AM researchers and practitioners with the latest information consequential for further development.",A systematic literature review on recent trends of machine learning applications in additive manufacturing,Journal of Intelligent Manufacturing,http://dx.doi.org/10.1007/s10845-022-01957-6,0.0
42,"This paper presents the development and analysis of artificial neural network (ANN) models for the nonlinear fractional-order (FO) point reactor kinetics model, FO Nordheim–Fuchs model, inverse FO point reactor kinetics model and FO constant delayed neutron production rate approximation model. These models represent the dynamics of a nuclear reactor with neutron transport modeled as subdiffusion. These FO models are nonlinear in nature, are comprised of a system of coupled fractional differential equations and integral equations, and are considered to be difficult for solving both analytically and numerically. The ANN models were developed using the data generated from these models. The work involves the iterative process of ANN learning with different combinations of layers and neurons. It is shown through extensive simulation studies that the developed ANN models faithfully capture the transient and steady-state dynamics of these FO models, thereby providing a satisfactory representation for the nonlinear subdiffusive process of neutron transport in a nuclear reactor.",Modeling nonlinear fractional-order subdiffusive dynamics in nuclear reactor with artificial neural networks,International Journal of Dynamics and Control,http://dx.doi.org/10.1007/s40435-022-01100-6,0.0
44,"Abstract Some unmanned aerial vehicles, micro-air vehicles, and small-scale wind turbines operate at Reynolds number values less than $$5 \times 10^5$$ 5 × 10 5 based on chord length. However, there are limited data sets characterizing the airfoil performance at Reynolds number spanning $$2\times 10^4 \le Re_c \le 5\times 10^4$$ 2 × 10 4 ≤ R e c ≤ 5 × 10 4 . The objective of this study is to investigate the impact of airfoil thickness and camber for canonical NACA airfoils at Reynolds numbers in this range and to correlate the observed aerodynamic behavior with the flow patterns. For this purpose, NACA-0009, 0012, 0021, and 6409 airfoils were used, and all experiments were performed in a water tunnel. A high-precision load cell was utilized to characterize the performance of the airfoils, and the hydrogen bubble flow visualization was used to assess the flow over the airfoils. The results showed that the airfoil thickness and camber significantly influence the aerodynamic performance and a strong dependence on the Reynolds number was observed. Symmetric NACA airfoils exhibited nonlinear lift behavior at Reynolds number below $$4\times 10^4$$ 4 × 10 4 as well as abrupt changes in lift values. The cambered airfoil showed some Reynolds number dependence but performed better than its symmetrical counterpart. The aerodynamic performance was correlated with the observed flow features around the airfoils. Graphical abstract ",Aerodynamic behavior and flow visualization on canonical NACA airfoils at low Reynolds number,Journal of Visualization,http://dx.doi.org/10.1007/s12650-023-00910-w,0.0
45,"Phononic crystals (PCs) consist of a periodic arrangement of inclusions in a matrix material, and have garnered a great deal of interest owing to a phenomenon known as band gap frequencies in which particular frequency ranges are not able to propagate through the PCs. The aim of this work is to study the effects of magneto-elastic coupling and other parameters such as randomness in geometrical properties, volume fraction and size of inclusions on longitudinal wave propagation and, in particular, on the appearance of stop-band frequencies. The results indicate that the most important parameters deciding whether a frequency is in a stop-band or a pass-band are the randomness in geometrical properties and piezomagnetic coupling. It was observed that piezomagnetic coupling can lead to a widening of the first stop-band range for a periodic microstructure. Moreover, while randomness in particle size leads to a stop-band range and reduced wave transmission in the second pass region, randomness in particle position leads to removal of the pass band ranges compared to periodic structures. Additionally, the influence of piezomagnetic coupling becomes insignificant in fully random structures.",Effects of randomness and piezomagnetic coupling on the appearance of stop-bands in heterogeneous magnetorheological elastomers,Archive of Applied Mechanics,http://dx.doi.org/10.1007/s00419-023-02437-w,0.0
47,"The analysis of large simple graphs with extreme values of the densities of edges and triangles has been extended to the statistical structure of typical graphs of fixed intermediate densities, by the use of large deviations of Erdős-Rényi graphs. We prove that the typical graph exhibits sharp singularities as the constraining densities vary between different curves of extreme values, and we determine the precise nature of the singularities. The extension to graphs with fixed densities of edges and k -cycles for odd $$k>3$$ k > 3 is straightforward and we note the simple changes in the proof.",Typical large graphs with given edge and triangle densities,Probability Theory and Related Fields,http://dx.doi.org/10.1007/s00440-023-01187-8,0.0
48,"The Baxter permuton is a random probability measure on the unit square which describes the scaling limit of uniform Baxter permutations. We determine an explicit formula for the density of the expectation of the Baxter permuton. This answers a question of Dokos and Pak (Online J Anal Comb 9:12, 2014). We also prove that all pattern densities of the Baxter permuton are strictly positive, distinguishing it from other permutons arising as scaling limits of pattern-avoiding permutations. Our proofs rely on a recent connection between the Baxter permuton and Liouville quantum gravity (LQG) coupled with the Schramm-Loewner evolution (SLE). The method works equally well for a two-parameter generalization of the Baxter permuton recently introduced by the first author, except that the density is not as explicit. This new family of permutons, called skew Brownian permuton , describes the scaling limit of a number of random constrained permutations. We finally observe that in the LQG/SLE framework, the expected proportion of inversions in a skew Brownian permuton equals $$\frac{\pi -2\theta }{2\pi }$$ π - 2 θ 2 π where $$\theta $$ θ is the so-called imaginary geometry angle between a certain pair of SLE curves.",Baxter permuton and Liouville quantum gravity,Probability Theory and Related Fields,http://dx.doi.org/10.1007/s00440-023-01193-w,0.0
33,"We consider the model of a transportation problem with the objective of finding a minimum-cost transportation plan for shipping a given commodity from a set of supply centers to the customers. Since the exact values of supply and demand and the exact transportation costs are not always available for real-world problems, we adopt the approach of interval programming to represent such uncertainty, resulting in the model of an interval transportation problem. The interval model assumes that lower and upper bounds on the data are given and the values can be independently perturbed within these bounds. In this paper, we provide an overview of conditions for checking basic properties of the interval transportation problems commonly studied in interval programming, such as weak and strong feasibility or optimality. We derive a condition for testing weak optimality of a solution in polynomial time by finding a suitable scenario of the problem. Further, we formulate a similar condition for testing strong optimality of a solution for transportation problems with interval supply and demand (and exact costs). Moreover, we also survey the results on computing the best and the worst optimal value. We build on an exact method for solving the NP-hard problem of computing the worst (finite) optimal value of the interval transportation problem based on a decomposition of the optimal solution set by complementary slackness. Finally, we conduct computational experiments to show that the method can be competitive with the state-of-the-art heuristic algorithms.","Interval transportation problem: feasibility, optimality and the worst optimal value",Central European Journal of Operations Research,http://dx.doi.org/10.1007/s10100-023-00841-9,0.0
25,"Voronoi diagrams on triangulated surfaces based on the geodesic metric play a key role in many applications of computer graphics. Previous methods of constructing such Voronoi diagrams generally depended on having an exact geodesic metric. However, exact geodesic computation is time-consuming and has high memory usage, limiting wider application of geodesic Voronoi diagrams (GVDs). In order to overcome this issue, instead of using exact methods, we reformulate a graph method based on Steiner point insertion, as an effective way to obtain geodesic distances. Further, since a bisector comprises hyperbolic and line segments, we utilize Apollonius diagrams to encode complicated structures, enabling Voronoi diagrams to encode a medial-axis surface for a dense set of boundary samples. Based on these strategies, we present an approximation algorithm for efficient Voronoi diagram construction on triangulated surfaces. We also suggest a measure for evaluating similarity of our results to the exact GVD. Although our GVD results are constructed using approximate geodesic distances, we can get GVD results similar to exact results by inserting Steiner points on triangle edges. Experimental results on many 3D models indicate the improved speed and memory requirements compared to previous leading methods.",An efficient algorithm for approximate Voronoi diagram construction on triangulated surfaces,Computational Visual Media,http://dx.doi.org/10.1007/s41095-022-0326-0,0.0
31,"Face recognition is a prevalent identity verification method, but it requires a liveness detection system to guard against face fraud from printed images and mobile phone photographs. It is challenging to guarantee low complexity and high accuracy of the face anti-spoofing model applied on the Android development board at the same time in existing research. On the Android development board, we construct a liveness detection system that is resistant to attacks from printed images and photographs of electronic devices such as tablets. In conjunction with the actual circumstance, we create a lightweight liveness detection algorithm based on near-infrared images. We utilize MTCNN to clip the near-infrared images in order to preserve the facial portions and reduce the calculated cost. After applying Gamma correction to weaken the impact of illumination on the faces, the facial images are subsequently incorporated into the enhanced ShuffleNet V2 model based on the MBConv and squeeze-excitation (SE) modules for secondary classification. We evaluate the performance of the enhanced model on the CBSR NIR Face Dataset, CASIA NIR-VIS 2.0 Face Database and Oulu-CASIA, achieving 98.50%, 99.87% and 100% accuracy, which is superior to the performance of the original ShuffleNet V2 model and state-of-the-art methods. Our model’s FLOPs and Params are 0.28 G and 2.17 M, respectively. Meanwhile, the liveness detection system based on Android has an exceptionally high level of security performance",Design and implementation of liveness detection system based on improved shufflenet V2,"Signal, Image and Video Processing",http://dx.doi.org/10.1007/s11760-023-02524-z,0.0
30,"Data augmentation effectively alleviates the over-fitting problem in convolutional neural network-based (CNN-based) models, especially in the limited dataset. However, the inconsistency problem between the augmented sample and its original label is still a critical challenge during the augmentation operation. In this paper, we propose a novel data augmentation scheme named Interpretability-Mask (IM), which exploits the interpretability of the classifier to obtain the most discriminative regions and preserve label invariance. Concretely, we first construct a set-based representation for a sample and its label by superpixel segmentation and the local interpretable model-agnostic explanations (LIME) operator. Secondly, the sample represented by the superpixel set is utilized to synthesize the region-level disturbance augmentation sample with a random removal strategy. Then, the label constructed by the most interpretive superpixel set is applied to maintain the consistency between the augmented sample and its original label. Lastly, the augmentation scheme will be randomly used to the training stage. Extensive experiments are conducted on challenging datasets. A significant improvement in classification performance has achieved with the IM scheme. On the CIFAR-10 dataset, the Top-1 error rate drops by 2.15% at most. On the CIFAR-100 dataset, the Top-1 error rate decreases by up to 3.69%. And the maximum decline of the Top-1 error rate is 3.35% on the Mini-ImageNet. Experimental results manifest the effectiveness and generality of the proposed method.",Interpretability-Mask: a label-preserving data augmentation scheme for better classification,"Signal, Image and Video Processing",http://dx.doi.org/10.1007/s11760-023-02497-z,0.0
10,"This paper presents simulations of the growth of stationary and rising vapour bubbles in an extend pool of liquid using an Interface Capturing Computational Fluid Dynamics (CFD) methodology coupled with a method for simulating interfacial mass transfer at the vapour-liquid interface. The model enables mechanistic prediction of the local rate of phase change at the vapour-liquid interface and is applicable to realistic cases involving two-phase mixtures with large density ratios. The simulation methodology is based on the Volume of Fluid (VOF) representation of the flow, whereby an interfacial region in which mass transfer occurs is implicitly identified by a phase indicator, in this case the volume fraction of liquid, which varies from the value pertaining to the “bulk” liquid to the value of the bulk vapour. The novel methodology proposed here has been implemented using the Finite Volume framework and solution methods typical of “industrial” CFD practice embedded in the OpenFOAM CFD toolbox. Simulations are validated via comparison against experimental observations of spherical bubble growth in zero gravity and of the growth of a rising bubble in normal gravity. The validation cases represent a severe test for Interface Capturing methodologies due to large density ratios, the presence of strong interfacial evaporation and upward bubble rise motion. Agreement of simulation results with measurements available in the literature demonstrates that the methodology detailed herein is applicable to modelling bubble growth driven by phase-change in real fluids.",Modelling of free bubble growth with Interface Capturing Computational Fluid Dynamics,Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0139-5,0.0
11,"The present study investigates the transport of dilute bubbles by transitional flow in a joining, cross-flow-type T-junction channel geometry with Reynolds numbers at the outlet branch from Re 3 = 600 to 1800 and an inlet volume flow rate ratio of 1. Bubbles with diameters between d b = 400 and 600 µm are considered. The schematic pattern of the single-phase flow is introduced based on streakline dye visualizations. Complex 3D flow due to the narrow channel design dominates the recirculation area and flow instabilities become important with increasing Reynolds number, which can be observed by the fading of dye intensity. A numerical method is presented with unsteady boundary conditions based on laser Doppler velocimetry measurements. Bubble trajectories are obtained by an Euler-Lagrange approach. Using high-speed shadowgraphy method combined with image processing, bubble sizes were measured, and bubble trajectories were evaluated. Experimental bubble trajectories and numerically predicted bubble positions show good agreement for Re 3 = 600, which is also the case with the dye visualization image. For higher Reynolds numbers, measurements of the bubble trajectories are reported and compared to dye visualization images. The increasing flow instabilities influence the bubble transport, resulting in large variations of bubble locations.",Experimental and numerical study on the transport of dilute bubbles in a T-junction channel flow,Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0156-4,0.0
12,"This paper presents a detailed experimental and numerical analysis of free-falling particle streams impacting a 45° inclined surface of differing materials. The particles used in this study were glass spheres with average diameters of 136 and 342 µm and a density of 2500 kg/m 3 . The three mass flow rates considered are 50, 150, and 250 grams per minute (gpm). The effect of wall material on the collision process was also analysed. Special attention was paid to the influence of wall roughness. Therefore, a plate of stainless steel with polished surface, an aluminium sheet, and a Perspex plate with similar properties to those of the rest of the wall sections were used. The experimental data were used to improve and validate a wall collision model in the frame of the Lagrangian approach. A new drag force formula that includes the effects of particle concentration as well as particle Reynolds number was implemented into commercially available codes from CFX4-4 package. It was found that the improved CFD model better predicted the experimental measurements for the particle rebound properties. The rough-wall model in these results showed greater effect on smaller particles than on larger particles. The results also showed that the improved CFD model predicted the velocity changes slightly better than the standard model, and this was confirmed by both the quantitative velocity comparisons and the qualitative concentration plots. Finally, the inclusion of the particle-particle collision was shown to be the dominant factor in providing the dispersion of the particles post collision. Without a sufficient particle-particle collision model, the standard model showed all particles behaving virtually identical, with the main particle stream continuing after the collision process.",Experimental and numerical study of free-falling streams of particles impacting an inclined surface,Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0144-8,0.0
13,"The ability to accurately model condensing flows is crucial for understanding such flows in many applications. Condensing flows of pure steam have been studied extensively in the past, and several droplet growth models have been derived. The rationale for the choice of growth models for condensation in humid air is less established. Furthermore, only a few validation cases for condensation in such flows exist. This paper aims to identify existing limitations of common droplet growth laws. The Hertz—Knudsen model is compared to heat-transfer-based models by Gyarmathy and Young, using an Euler—Lagrange approach in Ansys Fluent. For this, an adaption for Young’s growth law is introduced, allowing its application in condensation of different gas mixtures. The numerical model has been validated and applied to flows in nozzles and turbines in previous publications. The accuracy of the droplet growth models is investigated in transonic nozzle test cases. A case with pure steam and a case with humid air at two different humidity values are considered. Finally, the influence of humidity, Knudsen number, and droplet radius on the growth rate of each model is shown analytically. Flows at lower humidity values with longer condensation zones are shown to benefit from the higher sensitivity of the Hertz—Knudsen model to the mass fraction of water vapor in the flow. Heat-transfer-based models tend to overestimate condensation in such flows. However, the ability to empirically adapt the growth model by Young and its applicability in different Knudsen numbers results in good agreement with validation data over a wide range of cases.",Modeling condensing flows of humid air in transonic nozzles,Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0152-8,0.0
14,Abstract An algorithm has been developed and implemented for the numerical solution of a direct linear magnetostatics problem by calculating the resulting field of a pipe with a surface defect on its inner wall under the assumption that the perpendicular section of the pipe and the vector of intensity of the external magnetizing field remain unchanged along the pipe length axis. This has made it possible to take the two-dimensional integro-differential equation of magnetostatics as a basis. The algorithm is implemented in the FORTRAN programming language. The results are tested for reliability using problems solved exactly. Illustrative curves are constructed. The possibilities of applying the obtained calculation methodology to a class of problems that differ in some way in their formulation are indicated.,On the Solution to One Problem of Magnetostatics for a Pipe with Inner Surface Defect,Russian Journal of Nondestructive Testing,http://dx.doi.org/10.1134/S1061830923700274,0.0
15,,Contrast Media Research Symposium 2022,Molecular Imaging and Biology,http://dx.doi.org/10.1007/s11307-022-01785-3,0.0
16,"Increasingly, marine renewable energies are taking over as one of the most relevant solutions to minimize dependence on fossil fuels. The management and exploitation of such energy requires the optimization of converters that will, later on, ensure the conversion of hydraulic energy into electrical energy; among these converters are the oscillating water column. An OWC is characterized by its simplicity and its effectiveness against turbulent ocean conditions. The performance of OWCs depends strongly on the geometrical parameters of the air chamber such as: chamber walls, width, thickness of the front wall, slope at the bottom of the chamber and size of the opening. In this sense, the manuscript presents a parametric approach to investigate, by experimental tests, the hydrodynamic properties and the performance of oscillating water column wave energy converter (OWC). The effects of some geometrical key parameters of the system are analyzed. The tests are carried out on a small‐size OWC. The work seems to be interesting in view of its experimental aspect. We have realized a prototype of an oscillating water column (OWC) which consists of a box (an air chamber) having the shape of parallelepipeds. The experimental results found by this study showed different optimums of: (a) the distance between the wave generator and the device (2 positions). (b) The depth of water in the hydraulic channel. (c) The immersion depth of the front wall of the chamber. (d) The opening at the bottom of the prototype. The results obtained show that the coupling of the geometrical parameters of the device and the conditions of installation leads to an improvement of the hydrodynamic performances of the OWC. The study also shows that the various optimums found give a considerable increase in the energy output.",Experimental study of an oscillating water column wave energy converter based on regular waves,Marine Systems & Ocean Technology,http://dx.doi.org/10.1007/s40868-022-00121-2,0.0
19,"In this paper, we propose a correlation-aware probabilistic data summarization technique to efficiently analyze and visualize large-scale multi-block volume data generated by massively parallel scientific simulations. The core of our technique is correlation modeling of distribution representations of adjacent data blocks using copula functions and accurate data value estimation by combining numerical information, spatial location, and correlation distribution using Bayes’ rule. This effectively preserves statistical properties without merging data blocks in different parallel computing nodes and repartitioning them, thus significantly reducing the computational cost. Furthermore, this enables reconstruction of the original data more accurately than existing methods. We demonstrate the effectiveness of our technique using six datasets, with the largest having one billion grid points. The experimental results show that our approach reduces the data storage cost by approximately one order of magnitude compared to state-of-the-art methods while providing a higher reconstruction accuracy at a lower computational cost.",Correlation-aware probabilistic data summarization for large-scale multi-block scientific data visualization,Computational Visual Media,http://dx.doi.org/10.1007/s41095-022-0304-6,0.0
20,"Many dental procedures are aerosol-generating and pose a risk for the spread of airborne diseases, including COVID-19. Several aerosol mitigation strategies are available to reduce aerosol dispersion in dental clinics, such as increasing room ventilation and using extra-oral suction devices and high-efficiency particulate air (HEPA) filtration units. However, many questions remain unanswered, including what the optimal device flow rate is and how long after a patient exits the room it is safe to start treatment of the next patient. This study used computational fluid dynamics (CFD) to quantify the effectiveness of room ventilation, an HEPA filtration unit, and two extra-oral suction devices to reduce aerosols in a dental clinic. Aerosol concentration was quantified as the particulate matter under 10 µm (PM 10 ) using the particle size distribution generated during dental drilling. The simulations considered a 15 min procedure followed by a 30 min resting period. The efficiency of aerosol mitigation strategies was quantified by the scrubbing time, defined as the amount of time required to remove 95% of the aerosol released during the dental procedure. When no aerosol mitigation strategy was applied, PM 10 reached 30 µg/m 3 after 15 min of dental drilling, and then declined gradually to 0.2 µg/m 3 at the end of the resting period. The scrubbing time decreased from 20 to 5 min when the room ventilation increased from 6.3 to 18 air changes per hour (ACH), and decreased from 10 to 1 min when the flow rate of the HEPA filtration unit increased from 8 to 20 ACH. The CFD simulations also predicted that the extra-oral suction devices would capture 100% of the particles emanating from the patient’s mouth for device flow rates above 400 L/min. In summary, this study demonstrates that aerosol mitigation strategies can effectively reduce aerosol concentrations in dental clinics, which is expected to reduce the risk of spreading COVID-19 and other airborne diseases.",Quantifying strategies to minimize aerosol dispersion in dental clinics,Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0157-3,0.0
21,"Accurate modeling and simulation of metered-dose inhaler (MDI) drug delivery require detailed information about the spray aerosols and carrier airflows, which are sensitive to the geometry and formulation of the inhaler. This study aimed to systemically examine the effects of the MDI canister-holder guiding vanes and the orifice airflow on inhalation dosimetry. An MDI model was reconstructed from an actual inhaler that included a 0.5-mm-diameter orifice and six vertical guiding vanes on the inner wall of the canister-holder. Large-eddy simulation was used to capture the transient concurrent inspiratory and orifice airflows, and spray aerosols were tracked using the Lagrangian method. Measured aerosol size distribution and velocity were used to develop the computational model. Results show that MDI spray plume transport and deposition are sensitive to the instantaneous flow structures. Excluding the guiding vanes increased the mouth deposition by 8% (from 60% to 68%), while excluding the orifice jet flow decreased the mouth deposition by 5.5% (from 60% to 54.5%) compared to the control case. The impact of these two geometrical and flow details could persist in the small airways. The penetration rate to the left-lower lobe beyond the nineth generation (G9) increased by 67% when neglecting guiding vanes and increased by 50% when neglecting orifice flow.",Effects of guiding vanes and orifice jet flow of a metered-dose inhaler on drug dosimetry in human respiratory tract,Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0141-y,0.0
23,"This study focuses on the transport, deposition, and triggered immune response of intranasal vaccine droplets to the angiotensin-converting-enzyme-2-rich region, i.e., the olfactory region (OR), in the nasal cavity of a 6-year-old female to possibly prevent corona virus disease 19 (COVID-19). To investigate how administration strategy can influence nasal vaccine efficiency, a validated multi-scale model, i.e., computational fluid-particle dynamics (CFPD) and host-cell dynamics (HCD) model, was employed. Droplet deposition fraction, size change, residence time, and the area percentage of OR covered by the vaccine droplets, and triggered immune system response were predicted with different spray cone angles, initial droplet velocities, and compositions. Numerical results indicate that droplet initial velocity and composition have negligible influences on the vaccine delivery efficiency to OR. In contrast, the spray cone angle can significantly impact the vaccine delivery efficiency. The triggered immunity was not significantly influenced by the administration investigated in this study due to the low percentage of OR area covered by the droplets. To enhance the effectiveness of the intranasal vaccine to prevent COVID-19 infection, it is necessary to optimize the vaccine formulation and administration strategy so that the vaccine droplets can cover more epithelial cells in OR to minimize the number of available receptors for SARS-CoV-2.","Prediction of transport, deposition, and resultant immune response of nasal spray vaccine droplets using a CFPD-HCD model in a 6-year-old upper airway geometry to potentially prevent COVID-19",Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0145-7,0.0
24,"Treatment of sinusitis by surgical procedures is recommended only when medication therapies fail to relieve sinusitis symptoms. In this study, a realistic 3D model of the human upper airway system was constructed based on CT images of an adult male and three different virtual functional endoscopic sinus surgeries (FESS), including only uncinectomy and uncinectomy with two different sizes of Middle Meatal Antrostomy (MMA) performed on that model. Airflow and deposition of micro-particles in the range of 1–30 µm were numerically simulated in the postoperative cases for rest and moderate activity breathing conditions. The results showed that the uncinate process alone protects the maxillary sinus well against the entry of micro-particles, and its removal by uncinectomy allows particles to deposit on the sinus wall easily. Generally, uncinectomy with a degree of MMA increases the number of deposited particles in the maxillary sinuses compared to uncinectomy surgery alone. In the studied models, the highest particle deposition in the maxillary sinuses occurred among particles with a diameter of 10–20 µm. Also, if a person inhales particles during rest breathing conditions at a low respiratory rate, the number of particles deposited in the sinuses increases.",Micro-particle deposition in maxillary sinus for various sizes of opening in a virtual endoscopic surgery,Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0155-5,0.0
26,"Deep learning has been successfully used for tasks in the 2D image domain. Research on 3D computer vision and deep geometry learning has also attracted attention. Considerable achievements have been made regarding feature extraction and discrimination of 3D shapes. Following recent advances in deep generative models such as generative adversarial networks, effective generation of 3D shapes has become an active research topic. Unlike 2D images with a regular grid structure, 3D shapes have various representations, such as voxels, point clouds, meshes, and implicit functions. For deep learning of 3D shapes, shape representation has to be taken into account as there is no unified representation that can cover all tasks well. Factors such as the representativeness of geometry and topology often largely affect the quality of the generated 3D shapes. In this survey, we comprehensively review works on deep-learning-based 3D shape generation by classifying and discussing them in terms of the underlying shape representation and the architecture of the shape generator. The advantages and disadvantages of each class are further analyzed. We also consider the 3D shape datasets commonly used for shape generation. Finally, we present several potential research directions that hopefully can inspire future works on this topic.",A survey of deep learning-based 3D shape generation,Computational Visual Media,http://dx.doi.org/10.1007/s41095-022-0321-5,0.0
28,,Message from the Guest Editor of the SCONA 2022 Meeting Special Issue,Experimental and Computational Multiphase Flow,http://dx.doi.org/10.1007/s42757-022-0147-5,0.0
29,"By suitably adjusting the tropical algebra technique we compute the rainbow independent domination numbers of several infinite families of graphs including Cartesian products $$C_n \Box P_m$$ C n □ P m and $$C_n \Box C_m$$ C n □ C m for all n and $$m\le 5$$ m ≤ 5 , and generalized Petersen graphs P ( n , 2) for $$n \ge 3$$ n ≥ 3 .",On the 2-rainbow independent domination numbers of some graphs,Central European Journal of Operations Research,http://dx.doi.org/10.1007/s10100-023-00840-w,0.0
49,"Tread wear and rolling contact fatigue (RCF) damage propagated on railway wheels are the two extremely important focal points as they can tremendously deteriorate wheel/rail interactions and hunting stability and destroy wheel surface materials, and subsequently, cut down the lifetime of the wheels. The on-board anti-slip controllers are of essence aiming to hold back the striking slipping of the powered wheelsets under low-adhesion wheel/rail conditions. This paper intends to investigate the impact of anti-slip control on wheel tread wear and fatigue damage under diverse wheel/rail friction conditions. To this end, a prediction model for wheel wear and fatigue damage evolution on account of a comprehensive vehicle-track interaction model is extended, where the wheel/rail non-Hertzian contact algorithm is used. Furthermore, the effect of frictional wear on the fatigue damage at wheel surface is considered. The simulation results indicate that the wheel/rail contact is full-slip under the low-adhesion conditions with braking effort. The wear amount under the low-adhesion conditions is observably higher than that under the dry condition. It is further suggested that the wheel tread is prone to suffering more serious wear and fatigue damage issues with a higher anti-slip control threshold compared to that with a lower one.",A numerical study on tread wear and fatigue damage of railway wheels subjected to anti-slip control,Friction,http://dx.doi.org/10.1007/s40544-022-0684-8,0.0
